{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import openai\n",
    "import pinecone\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from nemoguardrails import LLMRails, RailsConfig\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\n",
    "    \"jamescalam/llama-2-arxiv-papers-chunked\",\n",
    "    split=\"train\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['doi', 'chunk-id', 'chunk', 'id', 'title', 'summary', 'source', 'authors', 'categories', 'comment', 'journal_ref', 'primary_category', 'published', 'updated', 'references', 'uid'],\n",
       "    num_rows: 4838\n",
       "})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.map(lambda x: {\n",
    "    'uid': f\"{x['doi']}-{x['chunk-id']}\"\n",
    "})\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>chunk</th>\n",
       "      <th>title</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1102.0183-0</td>\n",
       "      <td>High-Performance Neural Networks\\nfor Visual O...</td>\n",
       "      <td>High-Performance Neural Networks for Visual Ob...</td>\n",
       "      <td>http://arxiv.org/pdf/1102.0183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1102.0183-1</td>\n",
       "      <td>January 2011\\nAbstract\\nWe present a fast, ful...</td>\n",
       "      <td>High-Performance Neural Networks for Visual Ob...</td>\n",
       "      <td>http://arxiv.org/pdf/1102.0183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1102.0183-2</td>\n",
       "      <td>promising architectures for such tasks. The mo...</td>\n",
       "      <td>High-Performance Neural Networks for Visual Ob...</td>\n",
       "      <td>http://arxiv.org/pdf/1102.0183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1102.0183-3</td>\n",
       "      <td>Mutch and Lowe, 2008), whose \flters are \fxed, ...</td>\n",
       "      <td>High-Performance Neural Networks for Visual Ob...</td>\n",
       "      <td>http://arxiv.org/pdf/1102.0183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1102.0183-4</td>\n",
       "      <td>We evaluate various networks on the handwritte...</td>\n",
       "      <td>High-Performance Neural Networks for Visual Ob...</td>\n",
       "      <td>http://arxiv.org/pdf/1102.0183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4833</th>\n",
       "      <td>2307.09288-315</td>\n",
       "      <td>BytheCentralLimitTheorem, Zntendstowardsastand...</td>\n",
       "      <td>Llama 2: Open Foundation and Fine-Tuned Chat M...</td>\n",
       "      <td>http://arxiv.org/pdf/2307.09288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4834</th>\n",
       "      <td>2307.09288-316</td>\n",
       "      <td>Table 52 presents a model card (Mitchell et al...</td>\n",
       "      <td>Llama 2: Open Foundation and Fine-Tuned Chat M...</td>\n",
       "      <td>http://arxiv.org/pdf/2307.09288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4835</th>\n",
       "      <td>2307.09288-317</td>\n",
       "      <td>models will be released as we improve model sa...</td>\n",
       "      <td>Llama 2: Open Foundation and Fine-Tuned Chat M...</td>\n",
       "      <td>http://arxiv.org/pdf/2307.09288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4836</th>\n",
       "      <td>2307.09288-318</td>\n",
       "      <td>Training Factors We usedcustomtraininglibrarie...</td>\n",
       "      <td>Llama 2: Open Foundation and Fine-Tuned Chat M...</td>\n",
       "      <td>http://arxiv.org/pdf/2307.09288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4837</th>\n",
       "      <td>2307.09288-319</td>\n",
       "      <td>Evaluation Results\\nSee evaluations for pretra...</td>\n",
       "      <td>Llama 2: Open Foundation and Fine-Tuned Chat M...</td>\n",
       "      <td>http://arxiv.org/pdf/2307.09288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4838 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 uid                                              chunk  \\\n",
       "0        1102.0183-0  High-Performance Neural Networks\\nfor Visual O...   \n",
       "1        1102.0183-1  January 2011\\nAbstract\\nWe present a fast, ful...   \n",
       "2        1102.0183-2  promising architectures for such tasks. The mo...   \n",
       "3        1102.0183-3  Mutch and Lowe, 2008), whose \n",
       "lters are \n",
       "xed, ...   \n",
       "4        1102.0183-4  We evaluate various networks on the handwritte...   \n",
       "...              ...                                                ...   \n",
       "4833  2307.09288-315  BytheCentralLimitTheorem, Zntendstowardsastand...   \n",
       "4834  2307.09288-316  Table 52 presents a model card (Mitchell et al...   \n",
       "4835  2307.09288-317  models will be released as we improve model sa...   \n",
       "4836  2307.09288-318  Training Factors We usedcustomtraininglibrarie...   \n",
       "4837  2307.09288-319  Evaluation Results\\nSee evaluations for pretra...   \n",
       "\n",
       "                                                  title  \\\n",
       "0     High-Performance Neural Networks for Visual Ob...   \n",
       "1     High-Performance Neural Networks for Visual Ob...   \n",
       "2     High-Performance Neural Networks for Visual Ob...   \n",
       "3     High-Performance Neural Networks for Visual Ob...   \n",
       "4     High-Performance Neural Networks for Visual Ob...   \n",
       "...                                                 ...   \n",
       "4833  Llama 2: Open Foundation and Fine-Tuned Chat M...   \n",
       "4834  Llama 2: Open Foundation and Fine-Tuned Chat M...   \n",
       "4835  Llama 2: Open Foundation and Fine-Tuned Chat M...   \n",
       "4836  Llama 2: Open Foundation and Fine-Tuned Chat M...   \n",
       "4837  Llama 2: Open Foundation and Fine-Tuned Chat M...   \n",
       "\n",
       "                               source  \n",
       "0      http://arxiv.org/pdf/1102.0183  \n",
       "1      http://arxiv.org/pdf/1102.0183  \n",
       "2      http://arxiv.org/pdf/1102.0183  \n",
       "3      http://arxiv.org/pdf/1102.0183  \n",
       "4      http://arxiv.org/pdf/1102.0183  \n",
       "...                               ...  \n",
       "4833  http://arxiv.org/pdf/2307.09288  \n",
       "4834  http://arxiv.org/pdf/2307.09288  \n",
       "4835  http://arxiv.org/pdf/2307.09288  \n",
       "4836  http://arxiv.org/pdf/2307.09288  \n",
       "4837  http://arxiv.org/pdf/2307.09288  \n",
       "\n",
       "[4838 rows x 4 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[['uid', 'chunk', 'title', 'source']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'High-Performance Neural Networks\\nfor Visual Object Classi\\x0ccation\\nDan C. Cire\\x18 san, Ueli Meier, Jonathan Masci,\\nLuca M. Gambardella and J\\x7f urgen Schmidhuber\\nTechnical Report No. IDSIA-01-11\\nJanuary 2011\\nIDSIA / USI-SUPSI\\nDalle Molle Institute for Arti\\x0ccial Intelligence\\nGalleria 2, 6928 Manno, Switzerland\\nIDSIA is a joint institute of both University of Lugano (USI) and University of Applied Sciences of Southern Switzerland (SUPSI),\\nand was founded in 1988 by the Dalle Molle Foundation which promoted quality of life.\\nThis work was partially supported by the Swiss Commission for Technology and Innovation (CTI), Project n. 9688.1 IFF:\\nIntelligent Fill in Form.arXiv:1102.0183v1  [cs.AI]  1 Feb 2011\\nTechnical Report No. IDSIA-01-11 1\\nHigh-Performance Neural Networks\\nfor Visual Object Classi\\x0ccation\\nDan C. Cire\\x18 san, Ueli Meier, Jonathan Masci,\\nLuca M. Gambardella and J\\x7f urgen Schmidhuber\\nJanuary 2011\\nAbstract\\nWe present a fast, fully parameterizable GPU implementation of Convolutional Neural\\nNetwork variants. Our feature extractors are neither carefully designed nor pre-wired, but'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['chunk'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = \"text-embedding-ada-002\"\n",
    "res = client.embeddings.create(\n",
    "    input=[\n",
    "        data['chunk'][0],\n",
    "        data['chunk'][1]\n",
    "    ],\n",
    "    model=embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res.data[0].embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = Pinecone(api_key=os.environ.get(\"PINECONE_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"nemo-guardrails-rag-action\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nemo-guardrails-rag-action']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc.list_indexes().names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        index_name,\n",
    "        dimension=len(res.data[0].embedding),\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-west-2\")\n",
    "    )\n",
    "\n",
    "    while not pc.describe_index(index_name).status('ready'):\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 4838}},\n",
       " 'total_vector_count': 4838}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = pc.Index(index_name)\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [05:12<00:00,  6.39s/it]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100  # how many embeddings we create and insert at once\n",
    "\n",
    "for i in tqdm(range(0, len(data), batch_size)):\n",
    "    # find end of batch\n",
    "    i_end = min(len(data), i+batch_size)\n",
    "    batch = data[i:i_end]\n",
    "    # get ids\n",
    "    ids_batch = batch['uid'].to_list()\n",
    "    # get texts to encode\n",
    "    texts = batch['chunk'].to_list()\n",
    "    # create embeddings\n",
    "    res = res = client.embeddings.create(\n",
    "    input=texts,\n",
    "    model=embed_model\n",
    ")\n",
    "    embeds = [record.embedding for record in res.data]\n",
    "    # create metadata\n",
    "    metadata = [{\n",
    "        'chunk': x['chunk'],\n",
    "        'source': x['source']\n",
    "    } for _, x in batch.iterrows()]\n",
    "    to_upsert = list(zip(ids_batch, embeds, metadata))\n",
    "    # upsert to Pinecone\n",
    "    index.upsert(vectors=to_upsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 4838}},\n",
       " 'total_vector_count': 4838}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TEST PINECONE VEC STORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "xq = client.embeddings.create(\n",
    "    input=['tell me about llama 2'],\n",
    "    model=\"text-embedding-ada-002\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xq.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = index.query(\n",
    "    vector=xq.data[0].embedding,\n",
    "    top_k=3,\n",
    "    include_metadata=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l['matches'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts=[x['metadata']['chunk'] for x in l['matches']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def retrieve(query: str) -> list:\n",
    "    embed_query = client.embeddings.create(input=[query], model=embed_model)\n",
    "    query_emdns = embed_query.data[0].embedding\n",
    "    res = index.query(vector=query_emdns, top_k=5, include_metadata=True)\n",
    "    contexts=[x['metadata']['chunk'] for x in res['matches']]\n",
    "    return contexts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "use = await retrieve('llama 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use langchain to create prompt template\n",
    "async def rag(query: str, contexts: list) -> str:\n",
    "    print(\"> RAG CALLED\")\n",
    "    context_str = '\\n'.join(contexts)\n",
    "    system = f\"\"\"You are a helpful assistant designed to output JSON. Given a query from a user and\n",
    "    some relevant contexts. Answer the question given the information in those\n",
    "    contexts. If you cannot find the answer to the question, say \"I don't know\".\n",
    "    \"\"\"\n",
    "    user = f\"\"\"\n",
    "    {context_str}\n",
    "\n",
    "    Query: {query}\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\":\"system\", \"content\":system},\n",
    "        {\"role\":\"user\", \"content\":user}\n",
    "    ]\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-0125\",\n",
    "        response_format={ \"type\": \"json_object\" },\n",
    "        messages=messages\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> RAG CALLED\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\\n    \"answer\": \"Llama 2 is a collection of pretrained and fine-tuned large language models ranging from 7 billion to 70 billion parameters. The fine-tuned LLMs in Llama 2 are optimized for dialogue use cases and outperform open-source chat models on most benchmarks. They may be a suitable substitute for closed-source models based on humane evaluations for helpfulness and safety.\",\\n    \"source\": \"Llama 2 abstract\"\\n}'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await rag(query=\"tell me about llama 2\", contexts=use)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GUARDRAILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_content = \"\"\"\n",
    "models:\n",
    "- type: main\n",
    "  engine: openai\n",
    "  model: gpt-3.5-turbo-0125\n",
    "\"\"\"\n",
    "\n",
    "colang_content = \"\"\"\n",
    "# define limits\n",
    "define user ask politics\n",
    "    \"what are your political beliefs?\"\n",
    "    \"thoughts on the president?\"\n",
    "    \"left wing\"\n",
    "    \"right wing\"\n",
    "\n",
    "define bot answer politics\n",
    "    \"I'm a simple assistant, I don't like to talk of politics.\"\n",
    "    \"Sorry I can't talk about politics!\"\n",
    "\n",
    "define flow politics\n",
    "    user ask politics\n",
    "    bot answer politics\n",
    "    bot offer help\n",
    "\n",
    "# define RAG intents and flow\n",
    "define user ask llm\n",
    "    \"tell me about llama 2?\"\n",
    "    \"what is large language model\"\n",
    "    \"where did meta's new model come from?\" \n",
    "    \"how to llama?\"\n",
    "    \"have you ever meta llama?\"\n",
    "\n",
    "define flow llm\n",
    "    user ask llm\n",
    "    $contexts = execute retrieve(query=$last_user_message)\n",
    "    $answer = execute rag(query=$last_user_message, contexts=$contexts)\n",
    "    bot $answer\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = RailsConfig.from_content(\n",
    "    colang_content=colang_content,\n",
    "    yaml_content = yaml_content\n",
    ") \n",
    "rails = LLMRails(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "rails.register_action(action=retrieve, name=\"retrieve\")\n",
    "rails.register_action(action=rag, name=\"rag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await rails.generate_async(prompt='Hello')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> RAG CALLED\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\\n    \"answer\": \"Llama 2 is a collection of pretrained and fine-tuned large language models ranging from 7 billion to 70 billion parameters. The fine-tuned LLMs are optimized for dialogue use cases and outperform open-source chat models on most benchmarks tested. They may be a suitable substitute for closed-source models based on human evaluations for helpfulness and safety. Llama 2 models are developed and released by GenAI and Meta.\",\\n    \"source\": \"GenAI, Meta\"\\n}'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await rails.generate_async(prompt='tell me about llama 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents-0y0_WJic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
