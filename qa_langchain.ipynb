{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser\n",
    "\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
    "from langchain.text_splitter import CharacterTextSplitter, TokenTextSplitter\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.organization = os.getenv(\"OPENAI_ORG_ID\")\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QA SYSTEM\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEPS \n",
    "- Load documents\n",
    "- Split documents into semantic meaninful chunks \n",
    "\n",
    "\n",
    "\n",
    "- Create embeddings for documnets\n",
    "- Store embeddings in vector database (lookup similar vectors)\n",
    "- Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.0, model='gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/Users/Labi/V_env/assistant/QA_system/MLOps.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(file_path=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = TokenTextSplitter(\n",
    "    # separator=\"\\n\",\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=150,\n",
    "    # length_function=len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â splitting documents into small semantic meaningful chunks\n",
    "pages=text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = '/Users/Labi/V_env/assistant/QA_system/chroma'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "__import__('pysqlite3')\n",
    "import sys\n",
    "sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_db = Chroma.from_documents(\n",
    "    documents=pages,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'what are the principles and components of mlops'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_db.similarity_search(query, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = chroma_db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_stuff = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm, \n",
    "    # return_source_documents=True,\n",
    "    # chain_type=\"stuff\", \n",
    "    retriever=retriever, \n",
    "    # verbose=True,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = qa_stuff({'question':query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The principles of MLOps identified in the research are as follows:\\n\\n1. CI/CD automation: Continuous integration, continuous delivery, and continuous deployment to increase productivity.\\n2. Workflow orchestration: Coordinating ML workflow tasks according to directed acyclic graphs (DAGs) to manage dependencies.\\n3. Reproducibility: Ability to reproduce ML experiments and obtain the same results.\\n4. Versioning: Version control for data, model, and code to ensure reproducibility and traceability.\\n5. Collaboration: Collaborative work culture and tools to reduce domain silos and foster communication.\\n6. Continuous ML training & evaluation: Periodic retraining of ML models based on new data and evaluating model performance.\\n7. ML metadata tracking/logging: Tracking and logging metadata for tasks in the ML workflow pipeline to ensure traceability.\\n8. Continuous monitoring: Periodically assessing data, model, code, and infrastructure resources for potential errors or changes.\\n9. Feedback loops: Integrating insights from quality assessment into the development process and monitoring component.\\n\\nThe components associated with these principles include CI/CD automation, workflow orchestration, reproducibility, versioning, collaboration, continuous ML training and evaluation, ML metadata tracking/logging, continuous monitoring, and feedback loops. These components work together to automate and operationalize ML workflows and ensure the robustness and scalability of ML systems.'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent(\n",
    "        model_name,\n",
    "        file, \n",
    "        chain_type, \n",
    "        persist_directory, \n",
    "        k\n",
    "):\n",
    "    # load documents\n",
    "    loader = PyPDFLoader(file)\n",
    "    load_docs = loader.load()\n",
    "\n",
    "    # create doc chunks\n",
    "    text_splitter = TokenTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=150,\n",
    "    )\n",
    "    docs = text_splitter.split_documents(load_docs)\n",
    "\n",
    "    # define embedding\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "\n",
    "    # create vector database from data\n",
    "    vector_db = Chroma.from_documents(\n",
    "        documents=docs,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=persist_directory\n",
    "    )\n",
    "\n",
    "    # define retriever\n",
    "    retriever = vector_db.as_retriever(\n",
    "        search_type=\"similarity\", \n",
    "        search_kwargs={\"k\": k}\n",
    "    )\n",
    "\n",
    "    memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "    # create a chatbot chain. Memory is managed externally.\n",
    "    qa = ConversationalRetrievalChain.from_llm(\n",
    "        llm=ChatOpenAI(\n",
    "            model_name=model_name, \n",
    "            temperature=0\n",
    "        ), \n",
    "        chain_type=chain_type, \n",
    "        retriever=retriever, \n",
    "        memory=memory\n",
    "    )\n",
    "    return qa \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def respond(\n",
    "        agent\n",
    "):\n",
    "    chat = True\n",
    "    while chat:\n",
    "        query = input()\n",
    "        if query == 'exit':\n",
    "            chat = False\n",
    "        else:\n",
    "            response=agent({'question':query})\n",
    "            print(response['answer'])\n",
    "            print('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = respond(\n",
    "    agent=agent(\n",
    "        model_name='gpt-3.5-turbo',\n",
    "        file=file_path,\n",
    "        chain_type='stuff',\n",
    "        persist_directory=persist_directory,\n",
    "        k=2\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-8WonY2UZ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
